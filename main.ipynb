{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from police_reports import get_rdd as get_accident_rdd\n",
    "from Three_One_One import get_rdd as get_three_one_one_rdd\n",
    "from vehicle_volume_count import get_rdd as get_vehicle_rdd\n",
    "\n",
    "accident_rdd = get_accident_rdd(sc, \"NYPD_Motor_Vehicle_Collisions.csv\", download=True, output_path=\"Raw Result\")\n",
    "three_one_one_rdd = get_three_one_one_rdd(sc, \"311_Service_Requests_from_2010_to_Present.csv\", download=True, output_path=\"Raw Result\")\n",
    "count_rdd = get_vehicle_rdd(sc, \"zip_veh_count.csv\", download=True, output_path=\"Raw Result\")\n",
    "\n",
    "joined_rdd = count_rdd.join(accident_rdd.join(three_one_one_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapper(tuples):\n",
    "    for t in tuples:\n",
    "        zip_code = t[0]\n",
    "        traffic_count = t[1][0]\n",
    "        acc_t = t[1][1][0]\n",
    "        three_t = t[1][1][1]\n",
    "        tuple_type = type(())\n",
    "\n",
    "        if type(three_t) != tuple_type  or type(acc_t) != tuple_type:\n",
    "            continue\n",
    "        # accident dataset\n",
    "        t_accidents  = acc_t[0]\n",
    "        t_vehicles   = acc_t[1]\n",
    "        t_per_i      = acc_t[2]\n",
    "        t_per_k      = acc_t[3]\n",
    "        t_ped_i      = acc_t[4]\n",
    "        t_ped_k      = acc_t[5]\n",
    "        t_cyc_i      = acc_t[6]\n",
    "        t_cyc_k      = acc_t[7]\n",
    "        t_mot_i      = acc_t[8]\n",
    "        t_mot_k      = acc_t[9]\n",
    "        fac1         = acc_t[10]\n",
    "        fac2         = acc_t[11]\n",
    "        fac3         = acc_t[12]\n",
    "        fac4         = acc_t[13]\n",
    "        fac5         = acc_t[14]\n",
    "        veh1         = acc_t[15]\n",
    "        veh2         = acc_t[16]\n",
    "        veh3         = acc_t[17]\n",
    "        veh4         = acc_t[18]\n",
    "        veh5         = acc_t[19]\n",
    "        # 311 dataset\n",
    "        t_complaints = three_t[0]\n",
    "        comp1        = three_t[1]\n",
    "        comp2        = three_t[2]\n",
    "        comp3        = three_t[3]\n",
    "        desc1        = three_t[4]\n",
    "        desc2        = three_t[5]\n",
    "        desc3        = three_t[6]\n",
    "        desc4        = three_t[7]\n",
    "        desc5        = three_t[8]\n",
    "        city         = three_t[9]\n",
    "        \n",
    "        accidents_per_1000_vehicle = round(t_accidents / float(traffic_count) * 1000, 2)\n",
    "        injuries_per_1000_accident = round(t_per_i / float(t_accidents) * 1000, 2)\n",
    "        deaths_per_1000_accident   = round(t_per_k / float(t_accidents) * 1000, 2)\n",
    "        vehicles_involved_per_accident = round(t_vehicles / float(t_accidents), 2)\n",
    "        yield (city, zip_code, accidents_per_1000_vehicle, injuries_per_1000_accident, deaths_per_1000_accident, vehicles_involved_per_accident,\n",
    "              t_accidents, t_vehicles, fac1, fac2, fac3, fac4, fac5, veh1, veh2, veh3, veh4, veh5,\n",
    "              comp1, comp2, comp3, desc1, desc2, desc3, desc4, desc5, t_complaints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_rdd = joined_rdd.mapPartitions(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "fields = [StructField(\"\", StringType(), True) for _ in range(0, 27)]\n",
    "fields[0].name = \"City\"\n",
    "\n",
    "fields[1].name = \"Zip_Code\"\n",
    "\n",
    "fields[2].dataType = FloatType()\n",
    "fields[2].name = \"Accidents_Per_1000_Vehicles\"\n",
    "\n",
    "fields[3].dataType = FloatType()\n",
    "fields[3].name = \"Injuries_Per_1000_Accidents\"\n",
    "\n",
    "fields[4].dataType = FloatType()\n",
    "fields[4].name = \"Deaths_Per_1000_Accidents\"\n",
    "\n",
    "fields[5].dataType = FloatType()\n",
    "fields[5].name = \"Vehicles_Involved_Per_Accident\"\n",
    "\n",
    "fields[6].dataType = IntegerType()\n",
    "fields[6].name = \"Total_Number_of_Accidents\"\n",
    "\n",
    "fields[7].dataType = IntegerType()\n",
    "fields[7].name = \"Total_Number_of_Vehicles_Involved_in_Accidents\"\n",
    "\n",
    "fields[8].name  = \"Top_Factor_1\"\n",
    "fields[9].name  = \"Top_Factor_2\"\n",
    "fields[10].name  = \"Top_Factor_3\"\n",
    "fields[11].name = \"Top_Factor_4\"\n",
    "fields[12].name = \"Top_Factor_5\"\n",
    "\n",
    "fields[13].name = \"Most_Common_Vehicle_Type_1\"\n",
    "fields[14].name = \"Most_Common_Vehicle_Type_2\"\n",
    "fields[15].name = \"Most_Common_Vehicle_Type_3\"\n",
    "fields[16].name = \"Most_Common_Vehicle_Type_4\"\n",
    "fields[17].name = \"Most_Common_Vehicle_Type_5\"\n",
    "\n",
    "fields[18].name = \"Top_Street_Complaint_1\"\n",
    "fields[19].name = \"Top_Street_Complaint_2\"\n",
    "fields[20].name = \"Top_Street_Complaint_3\"\n",
    "\n",
    "fields[21].name = \"Top_Complaint_Description_1\"\n",
    "fields[22].name = \"Top_Complaint_Description_2\"\n",
    "fields[23].name = \"Top_Complaint_Description_3\"\n",
    "fields[24].name = \"Top_Complaint_Description_4\"\n",
    "fields[25].name = \"Top_Complaint_Description_5\"\n",
    "\n",
    "fields[26].dataType = IntegerType()\n",
    "fields[26].name = \"Total Street Complaints\"\n",
    "\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = sqlContext.createDataFrame(full_rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = full_df.describe(['Accidents_Per_1000_Vehicles', 'Injuries_Per_1000_Accidents', 'Deaths_Per_1000_Accidents', 'Vehicles_Involved_Per_Accident'])\n",
    "new_df = temp_df.select(temp_df.summary,\n",
    "                        temp_df.Accidents_Per_1000_Vehicles.cast(FloatType()).alias('Accidents'),\n",
    "                        temp_df.Injuries_Per_1000_Accidents.cast(FloatType()).alias('Injuries'),\n",
    "                        temp_df.Deaths_Per_1000_Accidents.cast(FloatType()).alias('Deaths'),\n",
    "                        temp_df.Vehicles_Involved_Per_Accident.cast(FloatType()).alias('Vehicles'))\n",
    " \n",
    "temp_lst = new_df.collect()\n",
    "\n",
    "avg_row = temp_lst[1]\n",
    "avg_accidents_per_1000             = round(avg_row.Accidents, 4)\n",
    "avg_injuries_per_1000              = round(avg_row.Injuries, 4)\n",
    "avg_deaths_per_1000                = round(avg_row.Deaths, 4)\n",
    "avg_vehicles_involved_per_accident = round(avg_row.Vehicles, 4)\n",
    "std_dev_row = temp_lst[2]\n",
    "std_dev_accidents_per_1000             = round(std_dev_row.Accidents, 4)\n",
    "std_dev_injuries_per_1000              = round(std_dev_row.Injuries, 4)\n",
    "std_dev_deaths_per_1000                = round(std_dev_row.Deaths, 4)\n",
    "std_dev_vehicles_involved_per_accident = round(std_dev_row.Vehicles, 4)\n",
    "\n",
    "one_std_dev_accidents   = std_dev_accidents_per_1000\n",
    "two_std_dev_accidents   = 2 * one_std_dev_accidents\n",
    "three_std_dev_accidents = 3 * one_std_dev_accidents\n",
    "\n",
    "one_std_dev_injuries    = std_dev_injuries_per_1000\n",
    "two_std_dev_injuries    = 2 * one_std_dev_injuries\n",
    "three_std_dev_injuries  = 3 * one_std_dev_injuries\n",
    "        \n",
    "one_std_dev_deaths      = std_dev_deaths_per_1000\n",
    "two_std_dev_deaths      = 2 * one_std_dev_deaths\n",
    "three_std_dev_deaths    = 3 * one_std_dev_deaths\n",
    "        \n",
    "one_std_dev_vehicles    = std_dev_vehicles_involved_per_accident\n",
    "two_std_dev_vehicles    = 2 * one_std_dev_vehicles\n",
    "three_std_dev_vehicles  = 3 * one_std_dev_vehicles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_10_accidents_per_1000_vehicles       = map(lambda x: x.asDict(), full_df.sort('Accidents_Per_1000_Vehicles', ascending=False).take(10))\n",
    "top_10_injuries_per_1000_accidents       = map(lambda x: x.asDict(), full_df.sort('Injuries_Per_1000_Accidents', ascending=False).take(10))\n",
    "top_10_deaths_per_1000_accidents         = map(lambda x: x.asDict(), full_df.sort('Deaths_Per_1000_Accidents', ascending=False).take(10))\n",
    "top_10_vehicles_involved_per_accident    = map(lambda x: x.asDict(), full_df.sort('Vehicles_Involved_Per_Accident', ascending=False).take(10))\n",
    "\n",
    "bottom_10_accidents_per_1000_vehicles    = map(lambda x: x.asDict(), full_df.sort('Accidents_Per_1000_Vehicles', ascending=True).take(10))\n",
    "bottom_10_injuries_per_1000_accidents    = map(lambda x: x.asDict(), full_df.sort('Injuries_Per_1000_Accidents', ascending=True).take(10))\n",
    "bottom_10_deaths_per_1000_accidents      = map(lambda x: x.asDict(), full_df.sort('Deaths_Per_1000_Accidents', ascending=True).take(10))\n",
    "bottom_10_vehicles_involved_per_accident = map(lambda x: x.asDict(), full_df.sort('Vehicles_Involved_Per_Accident', ascending=True).take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_mapper(records):\n",
    "    for record in records:\n",
    "        city = record[0]\n",
    "        zip_code = record[1]\n",
    "        accidents_per_1000 = record[2]\n",
    "        injuries_per_1000 = record[3]\n",
    "        deaths_per_1000 = record[4]\n",
    "        vehicles_involved_per_accident = record[5]\n",
    "        \n",
    "        upper_a_bound = round(avg_accidents_per_1000 + one_std_dev_accidents, 4)\n",
    "        lower_a_bound = round(avg_accidents_per_1000 - one_std_dev_accidents, 4)\n",
    "        \n",
    "        upper_i_bound = round(avg_injuries_per_1000 + one_std_dev_injuries, 4)\n",
    "        lower_i_bound = round(avg_injuries_per_1000 - one_std_dev_injuries, 4)\n",
    "        \n",
    "        upper_d_bound = round(avg_deaths_per_1000 + one_std_dev_deaths, 4)\n",
    "        lower_d_bound = round(avg_deaths_per_1000 - one_std_dev_deaths, 4)\n",
    "        \n",
    "        zip_safety_level = \"\"\n",
    "        \n",
    "        if accidents_per_1000 > upper_a_bound:\n",
    "            if injuries_per_1000 < lower_i_bound and deaths_per_1000 < lower_d_bound:\n",
    "                print (\"{} is classified as Ok with {} A {} I {} D \".format(zip_code, accidents_per_1000, injuries_per_1000, deaths_per_1000))\n",
    "                zip_safety_level = \"Ok\"\n",
    "            else:\n",
    "                print (\"{} is classified as Hazardous with {} accidents per 1000 vehicles\".format(zip_code, accidents_per_1000))\n",
    "                zip_safety_level = \"Hazardous\"\n",
    "        elif accidents_per_1000 < lower_a_bound:\n",
    "            if injuries_per_1000 > upper_i_bound and deaths_per_1000 > upper_d_bound:\n",
    "                print (\"{} is classified as Ok with {} A {} I {} D \".format(zip_code, accidents_per_1000, injuries_per_1000, deaths_per_1000))\n",
    "                zip_safety_level = \"Ok\"\n",
    "            else:\n",
    "                print(\"{} is classified as Safe with {} accidents per 1000 vehicles\".format(zip_code, accidents_per_1000))\n",
    "                zip_safety_level = \"Safe\"\n",
    "        else:\n",
    "            # accidents_per_1000 falls within one std deviation of mean\n",
    "            if injuries_per_1000 > upper_i_bound and deaths_per_1000 > upper_d_bound:\n",
    "                print(\"{} is classified as Hazardous with {} A {} I {} D \".format(zip_code, accidents_per_1000, injuries_per_1000, deaths_per_1000))\n",
    "                zip_safety_level = \"Hazardous\"\n",
    "            elif injuries_per_1000 < lower_i_bound and deaths_per_1000 < lower_d_bound:\n",
    "                print(\"{} is classified as Safe with {} A {} I {} D \".format(zip_code, accidents_per_1000, injuries_per_1000, deaths_per_1000))\n",
    "                zip_safety_level = \"Safe\"\n",
    "            else:\n",
    "                print (\"{} is classified as Ok with {} accidents per 1000 vehicles\".format(zip_code, accidents_per_1000))\n",
    "                zip_safety_level = \"Ok\"\n",
    "                \n",
    "        \n",
    "        yield (city, zip_code, zip_safety_level,\n",
    "              accidents_per_1000, injuries_per_1000, deaths_per_1000, vehicles_involved_per_accident,\n",
    "              record[6], record[7], record[8], record[9], record[10], record[11], record[12], record[13],\n",
    "              record[14], record[15], record[16], record[17], record[18], record[19], record[20], record[21],\n",
    "              record[22], record[23], record[24], record[25], record[26])\n",
    "\n",
    "        \n",
    "final_rdd = full_rdd.mapPartitions(classify_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toCSVLine(data):\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "final_rdd.map(toCSVLine).coalesce(1).saveAsTextFile(\"Raw Result/final_classification\")\n",
    "\n",
    "sc.parallelize(top_10_accidents_per_1000_vehicles).coalesce(1).saveAsTextFile(\"Raw Result/top_ten/top_ten_accidents_per_1000_vehicles\")\n",
    "sc.parallelize(top_10_injuries_per_1000_accidents).coalesce(1).saveAsTextFile(\"Raw Result/top_ten/top_ten_injuries_per_1000_accidents\")\n",
    "sc.parallelize(top_10_deaths_per_1000_accidents).coalesce(1).saveAsTextFile(\"Raw Result/top_ten/top_ten_deaths_per_1000_accidents\")\n",
    "sc.parallelize(top_10_vehicles_involved_per_accident).coalesce(1).saveAsTextFile(\"Raw Result/top_ten/top_ten_vehicles_involved_per_accident\")\n",
    "\n",
    "sc.parallelize(bottom_10_accidents_per_1000_vehicles).coalesce(1).saveAsTextFile(\"Raw Result/bottom_ten/bottom_ten_accidents_per_1000_vehicles\")\n",
    "sc.parallelize(bottom_10_injuries_per_1000_accidents).coalesce(1).saveAsTextFile(\"Raw Result/bottom_ten/bottom_ten_injuries_per_1000_accidents\")\n",
    "sc.parallelize(bottom_10_deaths_per_1000_accidents).coalesce(1).saveAsTextFile(\"Raw Result/bottom_ten/bottom_ten_deaths_per_1000_accidents\")\n",
    "sc.parallelize(bottom_10_vehicles_involved_per_accident).coalesce(1).saveAsTextFile(\"Raw Result/bottom_ten/bottom_ten_vehicles_involved_per_accident\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregation = map(lambda x: x.asDict(), new_df.select(new_df.summary,\n",
    "                                                      new_df.Accidents.alias(\"Accidents per 1000 vehicles\"),\n",
    "                                                      new_df.Injuries.alias(\"Injuries per 1000 accidents\"),\n",
    "                                                      new_df.Deaths.alias(\"Deaths per 1000 accidents\"),\n",
    "                                                      new_df.Vehicles.alias(\"Vehicles involved per accident\")).collect())\n",
    "\n",
    "\n",
    "sc.parallelize(aggregation).saveAsTextFile(\"Raw Result/aggregations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
